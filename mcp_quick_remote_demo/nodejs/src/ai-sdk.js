// ai-sdk.js - Vercel AI SDK with Lark OpenAPI MCP Demo
// ai-sdk.js - Vercel AI SDK ä¸ Lark OpenAPI MCP é€‚é…å™¨æ¼”ç¤º
// If you need more information about Vercel AI SDK, please refer to https://ai-sdk.dev/docs/introduction
// å¦‚æœä½ éœ€è¦æ›´å¤šå…³äº Vercel AI SDK çš„ä¿¡æ¯ï¼Œè¯·å‚è€ƒ https://ai-sdk.dev/docs/introduction

import { streamText } from "ai";
import { createMCPClient } from "@ai-sdk/mcp";
import { createOpenAICompatible } from "@ai-sdk/openai-compatible";
import { systemPrompt, userPrompt } from "./prompt.js";
import * as Lark from "@larksuiteoapi/node-sdk";

import dotenv from "dotenv";

// Load environment variables from .env file
// ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
dotenv.config();

// Validate required environment variables
// éªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡
if (!process.env.OPENAI_API_KEY || !process.env.OPENAI_MODEL) {
  throw new Error(
    "OPENAI_API_KEY, OPENAI_MODEL is required | OPENAI_API_KEY å’Œ OPENAI_MODEL æ˜¯å¿…éœ€çš„"
  );
}

// Initialize OpenAI-compatible model with custom headers
// ä½¿ç”¨è‡ªå®šä¹‰æ ‡å¤´åˆå§‹åŒ– OpenAI å…¼å®¹æ¨¡å‹
const model = createOpenAICompatible({
  baseURL: process.env.OPENAI_BASE_URL || "",
  name: process.env.OPENAI_MODEL || "",
  apiKey: process.env.OPENAI_API_KEY || "",
  headers: {
    "api-key": process.env.OPENAI_API_KEY || "", // Support for Azure OpenAI and other providers | æ”¯æŒ Azure OpenAI å’Œå…¶ä»–æä¾›å•†
  },
}).chatModel(process.env.OPENAI_MODEL || "");

/**
 * Create and configure Lark MCP client with http transport
 * ä½¿ç”¨ http ä¼ è¾“åˆ›å»ºå’Œé…ç½® Lark MCP å®¢æˆ·ç«¯
 *
 * @returns {Promise<MCPClient>} Configured MCP client | é…ç½®å¥½çš„ MCP å®¢æˆ·ç«¯
 */
async function createLarkMCPClient() {
  const client = new Lark.Client({
    appId: process.env.APP_ID,
    appSecret: process.env.APP_SECRET,
  });
  const tenantAccessToken = await client.tokenManager.getTenantAccessToken();

  const mcpUrl = process.env.MCP_URL || "https://mcp.feishu.cn/mcp";
  const allowedTools =
    process.env.LARK_MCP_ALLOWED_TOOLS || "create-doc,fetch-doc";

  const mcpClient = await createMCPClient({
    transport: {
      type: "http",
      url: mcpUrl,
      headers: {
        "X-Lark-MCP-Allowed-Tools": allowedTools,
        "X-Lark-MCP-TAT": tenantAccessToken,
      },
    },
  });
  return mcpClient;
}

/**
 * Main async function to run the AI SDK MCP demo with streaming
 * è¿è¡Œ AI SDK MCP æ¼”ç¤ºçš„ä¸»å¼‚æ­¥å‡½æ•°ï¼ˆæ”¯æŒæµå¼å¤„ç†ï¼‰
 */
async function main() {
  // Create MCP client and get available tools
  // åˆ›å»º MCP å®¢æˆ·ç«¯å¹¶è·å–å¯ç”¨å·¥å…·
  const mcpClient = await createLarkMCPClient();
  const tools = await mcpClient.tools();

  console.log("ğŸš€ è°ƒç”¨æµå¼æ–‡æœ¬ | Invoke streamText");

  const stream = streamText({
    model: model,
    system: systemPrompt,
    messages: [{ role: "user", content: userPrompt }],
    maxSteps: 10,
    tools,
    onChunk: (chunk) => {
      if (chunk.chunk.type === "text-delta") {
        process.stdout.write(chunk.chunk.textDelta);
      } else if (chunk.chunk.type === "tool-call") {
        console.log("ğŸ”§ Tool Call");
        console.log(chunk.chunk);
      } else if (chunk.chunk.type === "tool-result") {
        console.log("ğŸ”§ Tool Result");
        console.log(chunk.chunk);
      }
    },
    onStepFinish: () => {
      console.log("âœ… Step Finish");
    },
    onFinish: () => {
      console.log("âœ… All Finish");
    },
  });

  await stream.consumeStream();
  process.exit(0);
}

main();
