// ai-sdk.js - Vercel AI SDK with Lark OpenAPI MCP Demo
// ai-sdk.js - Vercel AI SDK ä¸ Lark OpenAPI MCP é€‚é…å™¨æ¼”ç¤º

// If you need more information about Vercel AI SDK, please refer to https://ai-sdk.dev/docs/introduction
// å¦‚æœä½ éœ€è¦æ›´å¤šå…³äº Vercel AI SDK çš„ä¿¡æ¯ï¼Œè¯·å‚è€ƒ https://ai-sdk.dev/docs/introduction

import {
  experimental_createMCPClient as createMCPClient,
  streamText,
} from "ai";
// @ts-ignore
import { Experimental_StdioMCPTransport as StdioMCPTransport } from "ai/mcp-stdio";
import { createOpenAICompatible } from "@ai-sdk/openai-compatible";
import { systemPrompt, userPrompt } from "./prompt.js";

import dotenv from "dotenv";

// Load environment variables from .env file
// ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
dotenv.config();

// Validate required environment variables
// éªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡
if (!process.env.OPENAI_API_KEY || !process.env.OPENAI_MODEL) {
  throw new Error(
    "OPENAI_API_KEY, OPENAI_MODEL is required | OPENAI_API_KEY å’Œ OPENAI_MODEL æ˜¯å¿…éœ€çš„"
  );
}

// Initialize OpenAI-compatible model with custom headers
// ä½¿ç”¨è‡ªå®šä¹‰æ ‡å¤´åˆå§‹åŒ– OpenAI å…¼å®¹æ¨¡å‹
const model = createOpenAICompatible({
  baseURL: process.env.OPENAI_BASE_URL || "",
  name: process.env.OPENAI_MODEL || "",
  apiKey: process.env.OPENAI_API_KEY || "",
  headers: {
    "api-key": process.env.OPENAI_API_KEY || "", // Support for Azure OpenAI and other providers | æ”¯æŒ Azure OpenAI å’Œå…¶ä»–æä¾›å•†
  },
}).chatModel(process.env.OPENAI_MODEL || "");

/**
 * Create and configure Lark MCP client with stdio transport
 * ä½¿ç”¨ stdio ä¼ è¾“åˆ›å»ºå’Œé…ç½® Lark MCP å®¢æˆ·ç«¯
 *
 * @returns {Promise<MCPClient>} Configured MCP client | é…ç½®å¥½çš„ MCP å®¢æˆ·ç«¯
 */
async function createLarkMCPClient() {
  let command = "npx";
  let args = [
    "-y",
    "@larksuiteoapi/lark-mcp",
    "mcp",
    "--token-mode",
    "tenant_access_token",
    // ä½ å¯ä»¥è‡ªå®šä¹‰å¼€å¯çš„ Tools æˆ–è€… Presets / You can custom enable tools or presets here
    // '-t',
    // 'bitable.v1.app.create,bitable.v1.appTable.create',
  ];

  // Use Windows platform to run npx command with cmd.exe | ä½¿ç”¨ Windows å¹³å°è¿è¡Œ npx å‘½ä»¤
  if (process.platform === "win32") {
    args = ["/c", command, ...args];
    command = "cmd.exe";
  }

  const transport = new StdioMCPTransport({
    transport: "stdio", // Use stdio for process communication | ä½¿ç”¨ stdio è¿›è¡Œè¿›ç¨‹é€šä¿¡
    command, // Use Node.js package runner | ä½¿ç”¨ Node.js åŒ…è¿è¡Œå™¨
    args, // Auto-yes and package name | è‡ªåŠ¨ç¡®è®¤å’ŒåŒ…å
    env: {
      // Lark app credentials for API access | ç”¨äº API è®¿é—®çš„é£ä¹¦/Larkåº”ç”¨å‡­è¯
      APP_ID: process.env.APP_ID,
      APP_SECRET: process.env.APP_SECRET,
      LARK_DOMAIN: process.env.LARK_DOMAIN, // Feishu/Lark request domain | é£ä¹¦/Lark API è¯·æ±‚åŸŸå
    },
  });
  return createMCPClient({ transport });
}

/**
 * Main async function to run the AI SDK MCP demo with streaming
 * è¿è¡Œ AI SDK MCP æ¼”ç¤ºçš„ä¸»å¼‚æ­¥å‡½æ•°ï¼ˆæ”¯æŒæµå¼å¤„ç†ï¼‰
 */
async function main() {
  // Create MCP client and get available tools
  // åˆ›å»º MCP å®¢æˆ·ç«¯å¹¶è·å–å¯ç”¨å·¥å…·
  const mcpClient = await createLarkMCPClient();
  const tools = await mcpClient.tools();

  console.log("ğŸš€ è°ƒç”¨æµå¼æ–‡æœ¬ | Invoke streamText");

  const stream = streamText({
    model: model,
    system: systemPrompt,
    messages: [{ role: "user", content: userPrompt }],
    maxSteps: 10,
    tools,
    onChunk: (chunk) => {
      if (chunk.chunk.type === "text-delta") {
        process.stdout.write(chunk.chunk.textDelta);
      } else if (chunk.chunk.type === "tool-call") {
        console.log("ğŸ”§ Tool Call");
        console.log(chunk.chunk);
      } else if (chunk.chunk.type === "tool-result") {
        console.log("ğŸ”§ Tool Result");
        console.log(chunk.chunk);
      }
    },
    onStepFinish: () => {
      console.log("âœ… Step Finish");
    },
    onFinish: () => {
      console.log("âœ… All Finish");
    },
  });

  await stream.consumeStream();
  process.exit(0);
}

main();
