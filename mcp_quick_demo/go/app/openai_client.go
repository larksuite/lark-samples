package app

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"mcp-quick-demo/prompt"
	"net/http"
	"os"
	"strings"

	"github.com/mark3labs/mcp-go/mcp"
	"github.com/sashabaranov/go-openai"
)

// customHeaderTransport is a custom HTTP Transport for adding custom headers to requests
// customHeaderTransport æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„ HTTP Transportï¼Œç”¨äºä¸ºè¯·æ±‚æ·»åŠ è‡ªå®šä¹‰å¤´éƒ¨
type customHeaderTransport struct {
	Transport http.RoundTripper // Base transport / åŸºç¡€ä¼ è¾“å±‚
	Headers   map[string]string // Custom headers to add / è¦æ·»åŠ çš„è‡ªå®šä¹‰å¤´éƒ¨
}

// RoundTrip implements the http.RoundTripper interface to add custom headers
// RoundTrip å®ç° http.RoundTripper æ¥å£ä»¥æ·»åŠ è‡ªå®šä¹‰å¤´éƒ¨
func (t *customHeaderTransport) RoundTrip(req *http.Request) (*http.Response, error) {
	// Add custom headers to each request / ä¸ºæ¯ä¸ªè¯·æ±‚æ·»åŠ è‡ªå®šä¹‰å¤´éƒ¨
	for key, value := range t.Headers {
		req.Header.Set(key, value)
	}
	return t.Transport.RoundTrip(req)
}

// ToolCall represents a tool function call request from the LLM
// ToolCall è¡¨ç¤ºæ¥è‡ª LLM çš„å·¥å…·å‡½æ•°è°ƒç”¨è¯·æ±‚
type ToolCall struct {
	ID        string                 `json:"id"`        // Unique identifier for the tool call / å·¥å…·è°ƒç”¨çš„å”¯ä¸€æ ‡è¯†ç¬¦
	Name      string                 `json:"name"`      // Name of the tool to call / è¦è°ƒç”¨çš„å·¥å…·åç§°
	Arguments map[string]interface{} `json:"arguments"` // Arguments to pass to the tool / ä¼ é€’ç»™å·¥å…·çš„å‚æ•°
}

// LLMClient defines the interface for interacting with Large Language Models
// LLMClient å®šä¹‰ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„æ¥å£
type LLMClient interface {
	// Chat starts a conversation with the given user query
	// Chat ä½¿ç”¨ç»™å®šçš„ç”¨æˆ·æŸ¥è¯¢å¼€å§‹å¯¹è¯
	Chat(ctx context.Context, query string)

	// ChatWithToolResult continues the conversation with tool execution results
	// ChatWithToolResult ä½¿ç”¨å·¥å…·æ‰§è¡Œç»“æœç»§ç»­å¯¹è¯
	ChatWithToolResult(ctx context.Context, toolCallID string, toolResult *mcp.CallToolResult) (response string, err error)

	// Continue processes the conversation and returns response with potential tool calls
	// Continue å¤„ç†å¯¹è¯å¹¶è¿”å›å“åº”ä»¥åŠæ½œåœ¨çš„å·¥å…·è°ƒç”¨
	Continue(ctx context.Context, availableTools []mcp.Tool) (response string, toolCalls []ToolCall, err error)
}

// OpenAILLMClient is an implementation of LLMClient using OpenAI's API
// OpenAILLMClient æ˜¯ä½¿ç”¨ OpenAI API çš„ LLMClient å®ç°
type OpenAILLMClient struct {
	client       *openai.Client                 // OpenAI API client / OpenAI API å®¢æˆ·ç«¯
	model        string                         // Model name to use / è¦ä½¿ç”¨çš„æ¨¡å‹åç§°
	conversation []openai.ChatCompletionMessage // Conversation history / å¯¹è¯å†å²
}

// NewLLMClient creates a new OpenAI LLM client with environment configuration
// NewLLMClient åˆ›å»ºä¸€ä¸ªæ–°çš„ä½¿ç”¨ç¯å¢ƒé…ç½®çš„ OpenAI LLM å®¢æˆ·ç«¯
func NewLLMClient() LLMClient {
	// Get API key from environment variable / ä»ç¯å¢ƒå˜é‡è·å– API å¯†é’¥
	apiKey := os.Getenv("OPENAI_API_KEY")
	if apiKey == "" {
		log.Fatal("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½® / OPENAI_API_KEY environment variable not set")
	}

	// Get model name from environment variable / ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°
	model := os.Getenv("OPENAI_MODEL")
	if model == "" {
		log.Fatal("OPENAI_MODEL ç¯å¢ƒå˜é‡æœªè®¾ç½® / OPENAI_MODEL environment variable not set")
	}

	// Create OpenAI client configuration / åˆ›å»º OpenAI å®¢æˆ·ç«¯é…ç½®
	config := openai.DefaultConfig(apiKey)

	// Set custom base URL if provided / å¦‚æœæä¾›äº†è‡ªå®šä¹‰åŸºç¡€ URL åˆ™è®¾ç½®
	if baseURL := os.Getenv("OPENAI_BASE_URL"); baseURL != "" {
		config.BaseURL = baseURL
	}

	// Create custom HTTP client with additional headers for compatibility
	// åˆ›å»ºå¸¦æœ‰é¢å¤–å¤´éƒ¨çš„è‡ªå®šä¹‰ HTTP å®¢æˆ·ç«¯ä»¥ç¡®ä¿å…¼å®¹æ€§
	customClient := &http.Client{
		Transport: &customHeaderTransport{
			Transport: http.DefaultTransport,
			Headers: map[string]string{
				"Authorization": "Bearer " + apiKey,
				"x-api-key":     apiKey,
				"Api-Key":       apiKey,
			},
		},
	}

	config.HTTPClient = customClient

	return &OpenAILLMClient{
		client: openai.NewClientWithConfig(config),
		model:  model,
		// Initialize conversation with system prompt / ä½¿ç”¨ç³»ç»Ÿæç¤ºåˆå§‹åŒ–å¯¹è¯
		conversation: []openai.ChatCompletionMessage{
			{
				Role:    openai.ChatMessageRoleSystem,
				Content: prompt.SystemPrompt,
			},
		},
	}
}

// convertMCPToolsToOpenAI converts MCP tool definitions to OpenAI tool format
// convertMCPToolsToOpenAI å°† MCP å·¥å…·å®šä¹‰è½¬æ¢ä¸º OpenAI å·¥å…·æ ¼å¼
func (c *OpenAILLMClient) convertMCPToolsToOpenAI(mcpTools []mcp.Tool) []openai.Tool {
	var openaiTools []openai.Tool

	for _, tool := range mcpTools {
		// Convert tool input schema to OpenAI parameters format
		// å°†å·¥å…·è¾“å…¥æ¶æ„è½¬æ¢ä¸º OpenAI å‚æ•°æ ¼å¼
		parametersJSON, _ := json.Marshal(tool.InputSchema)
		var parameters map[string]interface{}
		json.Unmarshal(parametersJSON, &parameters)

		openaiTool := openai.Tool{
			Type: openai.ToolTypeFunction,
			Function: &openai.FunctionDefinition{
				Name:        tool.Name,
				Description: tool.Description,
				Parameters:  parameters,
			},
		}
		openaiTools = append(openaiTools, openaiTool)
	}

	return openaiTools
}

// Continue processes the conversation and handles tool calls if needed
// Continue å¤„ç†å¯¹è¯å¹¶åœ¨éœ€è¦æ—¶å¤„ç†å·¥å…·è°ƒç”¨
func (c *OpenAILLMClient) Continue(ctx context.Context, availableTools []mcp.Tool) (string, []ToolCall, error) {
	// Convert MCP tools to OpenAI format / å°† MCP å·¥å…·è½¬æ¢ä¸º OpenAI æ ¼å¼
	openaiTools := c.convertMCPToolsToOpenAI(availableTools)

	// Prepare chat completion request / å‡†å¤‡èŠå¤©å®Œæˆè¯·æ±‚
	req := openai.ChatCompletionRequest{
		Model:    c.model,
		Messages: c.conversation,
		Tools:    openaiTools,
	}

	// Call OpenAI API / è°ƒç”¨ OpenAI API
	resp, err := c.client.CreateChatCompletion(ctx, req)
	if err != nil {
		return "", nil, fmt.Errorf("OpenAI APIè°ƒç”¨å¤±è´¥ / OpenAI API call failed: %w", err)
	}

	if len(resp.Choices) == 0 {
		return "", nil, fmt.Errorf("OpenAIè¿”å›ç©ºå“åº” / OpenAI returned empty response")
	}

	choice := resp.Choices[0]
	message := choice.Message

	// Add assistant's response to conversation history / å°†åŠ©æ‰‹çš„å“åº”æ·»åŠ åˆ°å¯¹è¯å†å²
	c.conversation = append(c.conversation, message)

	var toolCalls []ToolCall
	// Process tool calls if present / å¦‚æœå­˜åœ¨å·¥å…·è°ƒç”¨åˆ™å¤„ç†
	if len(message.ToolCalls) > 0 {
		for _, tc := range message.ToolCalls {
			// Parse tool call arguments / è§£æå·¥å…·è°ƒç”¨å‚æ•°
			var args map[string]interface{}
			if err := json.Unmarshal([]byte(tc.Function.Arguments), &args); err != nil {
				return "", nil, fmt.Errorf("è§£æå·¥å…·è°ƒç”¨å‚æ•°å¤±è´¥ / Failed to parse tool call arguments: %w", err)
			}

			toolCall := ToolCall{
				ID:        tc.ID,
				Name:      tc.Function.Name,
				Arguments: args,
			}
			toolCalls = append(toolCalls, toolCall)
		}
	}

	// Prepare response content / å‡†å¤‡å“åº”å†…å®¹
	responseContent := message.Content
	if responseContent == "" && len(toolCalls) > 0 {
		responseContent = "æˆ‘éœ€è¦è°ƒç”¨å·¥å…·æ¥å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚/ I need to call tools to process your request."
	}

	return responseContent, toolCalls, nil
}

// Chat adds a user message to the conversation history
// Chat å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
func (c *OpenAILLMClient) Chat(ctx context.Context, query string) {
	// Add user message to conversation history / æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
	userMessage := openai.ChatCompletionMessage{
		Role:    openai.ChatMessageRoleUser,
		Content: query,
	}
	c.conversation = append(c.conversation, userMessage)
}

// ChatWithToolResult adds tool execution results to the conversation and returns the result content
// ChatWithToolResult å°†å·¥å…·æ‰§è¡Œç»“æœæ·»åŠ åˆ°å¯¹è¯ä¸­å¹¶è¿”å›ç»“æœå†…å®¹
func (c *OpenAILLMClient) ChatWithToolResult(ctx context.Context, toolCallID string, toolResult *mcp.CallToolResult) (string, error) {
	var resultContent string

	// Extract text content from tool result / ä»å·¥å…·ç»“æœä¸­æå–æ–‡æœ¬å†…å®¹
	if len(toolResult.Content) > 0 {
		var contentParts []string
		for _, content := range toolResult.Content {
			if textContent, ok := mcp.AsTextContent(content); ok {
				contentParts = append(contentParts, textContent.Text)
			}
		}
		if len(contentParts) > 0 {
			resultContent = strings.Join(contentParts, "\n")
		} else {
			resultContent = "å·¥å…·è°ƒç”¨å®Œæˆï¼Œä½†æ²¡æœ‰è¿”å›æ–‡æœ¬å†…å®¹ã€‚/ Tool call completed but returned no text content."
		}
	} else {
		resultContent = "å·¥å…·è°ƒç”¨å®Œæˆï¼Œä½†æ²¡æœ‰è¿”å›å…·ä½“å†…å®¹ã€‚/ Tool call completed but returned no specific content."
	}

	// Add tool result message to conversation history / å°†å·¥å…·ç»“æœæ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²
	toolMessage := openai.ChatCompletionMessage{
		Role:       openai.ChatMessageRoleTool,
		Content:    resultContent,
		ToolCallID: toolCallID,
	}
	log.Printf("ğŸ”§ å·¥å…·è°ƒç”¨ç»“æœ / Tool call result: %+v", toolMessage)
	c.conversation = append(c.conversation, toolMessage)

	return resultContent, nil
}
